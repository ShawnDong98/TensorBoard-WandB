{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64fc5e77-bd54-434e-b591-e70a0ab957b9",
   "metadata": {},
   "source": [
    "# Visualizing Data using the Embedding Projector in TensorBoard\n",
    "\n",
    "## Overview\n",
    "\n",
    "使用TensorBoard Embedding Projector，您可以以图形方式表示高维嵌入。这有助于可视化、检查和理解您的嵌入层。\n",
    "\n",
    "## Setup\n",
    "\n",
    "在本教程中，我们将使用TensorBoard可视化为对电影评论数据进行分类而生成的嵌入层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03aa7b28-441b-4285-bd7b-f8bd7c9bd552",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a85aafeb-1c5a-4a36-9eda-0fb0c50c5e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (4.5.2)\n",
      "Requirement already satisfied: termcolor in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from tensorflow_datasets) (4.64.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from tensorflow_datasets) (1.7.0)\n",
      "Requirement already satisfied: numpy in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from tensorflow_datasets) (1.22.3)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from tensorflow_datasets) (3.19.4)\n",
      "Requirement already satisfied: six in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from tensorflow_datasets) (5.4.0)\n",
      "Requirement already satisfied: promise in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from tensorflow_datasets) (2.27.1)\n",
      "Requirement already satisfied: dill in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from tensorflow_datasets) (0.3.4)\n",
      "Requirement already satisfied: absl-py in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from tensorflow_datasets) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from importlib-resources->tensorflow_datasets) (3.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89507c0e-0ac7-4fb6-b2af-2d8a6f6e12b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shawnd/miniforge3/envs/keras/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b646f-6a05-474b-abfd-a9f19ad04d11",
   "metadata": {},
   "source": [
    "## IMDB Data\n",
    "\n",
    "我们将使用25,000条IMDB电影评论的数据集，每个评论都有一个情绪标签（正面/负面）。每个评论都经过预处理，并编码为一系列单词索引（整数）。为了简单起见，单词按数据集中的总体频率进行索引，例如整数“3”编码所有评论中出现的第3个最常见的单词。这允许快速过滤操作，例如：“只考虑前10,000个最常见的单词，但消除前20个最常见的单词”。\n",
    "\n",
    "按照惯例，“0”不代表任何特定单词，而是用于对任何未知单词进行编码。在教程的后面，我们将删除可视化中“0”的行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b156c2b6-78ce-4046-8455-620c347ed40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 11:37:18.165350: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-02 11:37:18.165454: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-05-02 11:37:18.206515: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-05-02 11:37:18.206670: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-05-02 11:37:18.232184: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "(train_data, test_data), info = tfds.load(\n",
    "    \"imdb_reviews/subwords8k\",\n",
    "    split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")\n",
    "encoder = info.features[\"text\"].encoder\n",
    "\n",
    "train_batches = train_data.shuffle(1000).padded_batch(\n",
    "    10, padded_shapes=((None,), ())\n",
    ")\n",
    "test_batches = test_data.shuffle(1000).padded_batch(\n",
    "    10, padded_shapes=((None,), ())\n",
    ")\n",
    "train_batch, train_labels = next(iter(train_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e055cf-73b5-42be-9ea0-48e1de7f0fcb",
   "metadata": {},
   "source": [
    "## Keras Embedding Layer\n",
    "\n",
    "Keras embedding 层可用于训练词汇表中每个单词的嵌入。每个单词（在这种情况下是子单词）将与模型训练的16维向量（或嵌入）相关联。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a0b239-710f-4af8-868a-701324e4b729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/2500 [..............................] - ETA: 11:57 - loss: 0.6927 - accuracy: 0.3000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 11:37:18.562654: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499/2500 [============================>.] - ETA: 0s - loss: 0.4977 - accuracy: 0.7073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 11:39:02.155129: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 104s 42ms/step - loss: 0.4976 - accuracy: 0.7074 - val_loss: 0.3417 - val_accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 11:39:02.646479: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "embedding = tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim)\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        embedding,\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_batches, epochs=1, validation_data=test_batches, validation_steps=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f2a4c-ba57-4d41-93bd-c19d2087f227",
   "metadata": {},
   "source": [
    "## Saving Data for TensorBoard\n",
    "\n",
    "TensorBoard 从 tensorboard 项目的日志中读取张量和元数据。日志目录的路径在下面用 `log_dir` 指定。在本教程中，我们将使用 `/logs/imdb-example/`。\n",
    "\n",
    "为了将数据加载到 Tensorboard 中，我们需要将训练检查点保存到该目录中，以及允许可视化模型中特定兴趣层的元数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cea31e81-eec4-49aa-bba7-c751b1dffcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './logs/imdb-example/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "    for subwords in encoder.subwords:\n",
    "        f.write(\"{}\\n\".format(subwords))\n",
    "    # Fill in the rest of the labels with \"unknown\".\n",
    "    for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):\n",
    "        f.write(\"unknown #{}\\n\".format(unknown))\n",
    "        \n",
    "weights = tf.Variable(model.layers[0].get_weights()[0][1:])\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = \"metadata.tsv\"\n",
    "projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a512fb2b-adfa-460d-b75c-ad6334b964ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 7108), started 0:04:03 ago. (Use '!kill 7108' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5515d90a0fd39b81\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5515d90a0fd39b81\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./logs/imdb-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebbbfb7-7179-40ee-a356-6dbb75ab84e9",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "TensorBoard Projector 是解释和可视化嵌入的绝佳工具。 Dashboard 允许用户搜索特定 terms ，并突出显示嵌入（低维）空间中相邻的单词。从这个例子中，我们可以看到 Wes Anderson 和 Alfred Hitchcock 都是相当中立的术语，但它们在不同的上下文中被引用。\n",
    "\n",
    "在这个空间中，`Hitchcock` 更接近 `nightmare` 等词语，这可能是因为他被称为 “Master of Suspense”，而 `Anderson` 更接近 `heart` 这个词，这与他  relentlessly detailed 和温馨风格一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c333c-611d-4a4a-9a8d-236c64eaf6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
